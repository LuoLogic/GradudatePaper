


\chapter{绪论}
\thispagestyle{others}
\pagestyle{others}
\xiaosi

=\section{研究背景及意义}
当今的社会正处于智能化趋势的浪潮中，由深度学习理论所衍生的相关技术被广泛的应用在人们所熟知的各个领域，例如自然语言处理\textsuperscript{\cite{language}}、语音识别\textsuperscript{\cite{voice}}、图像分类\textsuperscript{\cite{image,simonyan2014very}}、目标检测\textsuperscript{\cite{od1,od2}}和语义分割\textsuperscript{\cite{sg1,sg2}}。在学术界和工业界的互相融合促进下，深度学习算法不断推陈出新，深度神经网络也不断进化并发展出适用与文字、图像、视频等信息介质的自动化识别和信息提取的高效的深度神经网络模型，许多相关领域深度神经网络模型已经是现代社会正常运转的不可或缺的一部分。

但是，当前主流的深度神经网络并不具备良好的可解释性，即便这些深度神经网络在各种测试任务下展现出了很高的准确率。例如在图像分类应用中，将待分类的图片送入训练好的深度神经网络会得到不同类别物体的置信概率分数，即便某一类别的置信概率分数是99.99\%，也无法得到深度神经网络做出这一决策的依据\textsuperscript{\cite{machine}}，即该深度神经网络的输出结果并不具备可解释性。并且随着应用场景的复杂化多样化，深度神经网络结构日趋复杂，参数数量日趋庞大，这使得深度神经网络的“黑盒”特点变得更为突出。这种难以解释的黑盒特性使得深度神经网络在可靠性要求极高的领域，诸如医疗影像、自动驾驶、航空航天\textsuperscript{\cite{arospace}}等领域的应用就受到限制。

除此之外，上述的黑盒特性也会成为当前的深度神经网络的研究过程当中的“拦路虎”。研究者往往是将训练好的深度神经网络在既有的数据集上根据各种外部的量化指标评价训练效果，但是在实际应用过程当中，模型可能会对现实世界中某些特殊的数据或者人为恶意伪造的数据给出异常的结果。如果训练的深度神经网络不能对这些意外数据有着良好的鲁棒性，那么该神经网络的就不能得到人们的信任。因此这使得相关研究者必须从更加全面的角度判断深度神经网络的性能表现并且解释其训练的神经网络的结果输出，而不只是依赖单一的评价指标来判断神经网络的性能表现。

因此，从深度神经网络实际应用和可靠性的角度出发，都需要有适用于深度神经网络黑盒特性的可解释方法，所以许多研究人员将目光转移到深度神经网络的可解释性上，他们或试图从既有的深度神经网络内外寻找其的决策依据，或试图从原理上构建可解释的深度神经网络，这两类研究路径分别就是后解释（Post-hoc Explanation）\textsuperscript{\cite{post,post2}}的人工智能和自解释（Intrinsic Explanation）\textsuperscript{\cite{Intrinsic1,Intrinsic2,Intrinsic3,Intrinsic4,Intrinsic5,Intrinsic6}}的人工智能。

通过利用针对深度神经网络的可解释方法，不仅可以使得研究者和用户知道深度神经网络的决策依据，理解其中的决策机理，增强人对神经网络输出结果的信任程度，还可以使得研究者对神经网络的可靠性进行针对性的验证和测试。加强对深度神经网络可解释方法的研究有助于研究人员用更加全面且灵活双向的方式和神经网络进行交互，能做到“知其然并知其所以然”。可解释性的研究赋予了深度神经网络更多的可能性并加强了其可靠性。

本文主要聚焦于图像分类神经网络的可解释性研究，更加具体的是利用图像即显著图的方式来提供图像分类神经网络的可解释性，这是一种后解释的可解释研究，它能根据显著图中给出权值数据来解释图像分类神经网络的输出结果并判断其内部决策过程是否合理可靠。同时本文的研究侧重点在于不改变神经网络内部参数，仅利用既有训练好的深度神经网络模型来进行可解释性研究。既有的图像分类神经网络模型在工业界和学术界已经得到了大量的应用，并且拥有很好的性能表现，因此在不改变模型的前提下，本文的研究可以提供简单明了能直接使用的可解释方法，对已经训练完成模型的输出结果进行显著图分析解释，将单一的，不可解释的输出结果转变为直观明了的，可解释的图像结果，提高了模型的可靠性和可信任性。此外，显著图给出的分析结果可以帮助研究者有效分析模型是否学习到正确的特征，提供神经网络决策的关键依据。





\section{国内外研究现状}
\subsection{基于反向传播的显著图解释技术}
利用反向传播的机制来实现可视化解释是较早的一些工作所采用的方法。M D.Zeiler等人\textsuperscript{\cite{zeiler2014visualizing}}提出了一种基于反卷积的可视化方法，反卷积将特征值逆映射回了输入图片的像素空间，借此说明图片中的哪些像素参与激活了该特征值。在这项工作的基础上，导向反向传播方法\textsuperscript{\cite{springenberg2014striving}}提出在反向传播时通过抑制输入和梯度小于0的值，从而突出可视化目标的重要特征。DeepLIFT\textsuperscript{\cite{shrikumar2017learning}}，LRP(Layer-wise Relevance Propagation)\textsuperscript{\cite{binder2016layer}}方法通过修改反向传播的规则，将输出层的贡献逐渐向下分配直到输入层，以此来获得输入图片中每个像素对输出相关性分数。K.Simonyan等人\textsuperscript{\cite{simonyan2014deep}}提出使用输入的梯度作为可视化解释的一种手段，这种方法认为输入的某些像素对网络的预测结果起到了主要作用，它直接计算网络输出的特定类别分数对输入的梯度，但是输入的梯度中包含明显的噪声，导致显著图可视化十分模糊。SmoothGrad方法\textsuperscript{\cite{smilkov2017smoothgrad}}和VarGrad方法\textsuperscript{\cite{adebayo2018local}}的原理都是共同的，它们向输入图片中多次添加噪声生成一组包含噪声的图片，通过平均化结果使得生成的显著性图更加平滑。为了解决梯度饱和问题，M.Sundararajan等人提出了一种积分梯度方法\textsuperscript{\cite{sundararajan2017axiomatic}}，该方法结合了直接计算梯度和基于反向传播的归因技术DeepLIFT和LRP的分而治之的设计思想，满足敏感性和实现不变性的公理。这些研究虽然有坚实的理论基础，但是它们的可视化结果对于人类来说不容易理解，而且噪声较多。此外这些方法中许多是和具体类别不相干的，无法对指定类别给出显著图可视化解释结果。另外有研究\textsuperscript{\cite{adebayo2018sanity}}指出其中有些方法的可靠性是值得怀疑的，它们对深度神经网络的参数不敏感，即使网络没有经过训练也能得到相似的结果。

\subsection{基于类激活映射的显著图解释技术}
基于类激活映射的方法是目前被大量研究和应用的一种流行的方法。这类方法利用了卷积神经网络中靠近输出端的信息，这些信息中包含着和预测结果相关的丰富的类别信息，所以这类方法能够给出类别相关的显著图可视化解释。CAM\textsuperscript{\cite{zhou2016learning}}方法首先提出了将卷积神经网络的最后一层全局平均池化后得到权重和该层提取的特征图线性相乘后累加  从而生成显著图。Grad-CAM\textsuperscript{\cite{selvaraju2017grad}}对CAM方法进行了改进，无需修改网络结构，利用反向传播的梯度取均值作为权重。Grad-CAM++\textsuperscript{\cite{chattopadhay2018grad}}进一步改进了Grad-CAM,它对不同像素的梯度进行加权，生成的显著图中能将同一类别物体出现多次的情况给较好的展示出来。XGrad-CAM\textsuperscript{\cite{fu2020axiom}}通过分析敏感性和实现不变性公理采用了另一种加权方法来获得特征图的权重，它引入了特征图中的像素权重为对应梯度进一步加权。为了减少噪声的影响，Smooth Grad-CAM++\textsuperscript{\cite{omeiza2019smooth}}，SS-CAM\textsuperscript{\cite{wang2020ss}}也采用了SmoothGrad\textsuperscript{\cite{smilkov2017smoothgrad}}和VarGrad\textsuperscript{\cite{adebayo2018sanity}}中的向输入图片中多次添加噪声的措施。Score-CAM\textsuperscript{\cite{wang2020score}}和Ablation-CAM\textsuperscript{\cite{ramaswamy2020ablation}}没有使用反向传播中的梯度作为特征图的权重，它们将前向传播中从最后一层卷积层获得的特征图作为掩膜来扰动输入图片，利用网络输出值或者下降值作为特征图的权重，这种方法有效避免了使用梯度而产生的噪声，取得了良好的效果。Relevance-CAM\textsuperscript{\cite{lee2021relevance}}将卷积神经网络中提取的特征图再分别输入到卷积神经网络中，对每张特征图的预测结果进行层间相关性传播（Layer-wise Relevance Propagation），得到对应的相关性图，将相关性图全局平均池化后作为特征的权重，该方法有效解决了之前的CAM方法对卷积神经网络中间层可视化解释不足的问题。CAMERAS\textsuperscript{\cite{jalwana2021cameras}}提出了将输入图片进行多尺度放大再输入到网络当中，将提取到的不同分辨率的特征图和梯度全部放大到和原图分辨率一样。然而，基于类激活映射的方法只针对卷积神经网络进行设计，无法便捷的迁移到其他网络当中，比如基于Transformer的图片分类神经网络模型。


\subsection{基于扰动的显著图解释方法}
基于扰动的显著图可视化方法最突出的优点便是这种方法基本只关心网络的输入和输出，可以在不同结构网络轻松的应用，即使这些网络中的细节千差万别。M D.Zeiler等人\textsuperscript{\cite{zeiler2014visualizing}6-7}使用一个固定形状的方块来对输入图片进行扰动，观察输出变化从而找到对模型而已输入图片中最重要的部分。SHAP方法\textsuperscript{\cite{lundberg2017unified}}使用了Shapley值，计算不同输入像素在是否存在的情况下对网络预测结果的影响，公平分配贡献度给这些像素点。LIME\textsuperscript{\cite{ribeiro2016should}}方法通过在较小的范围内扰动输入图片，得到输出结果，利用输入数据和输出数据重新训练一个可解释的代理模型去逼近黑盒模型在局部的决策边界，从而获得不同特征的重要性。RISE\textsuperscript{\cite{petsiuk2018rise}}利用蒙特卡洛方法随机生成的数量巨大的掩膜来扰动输入图片，将模型对特定类别输出概率作为的权重，再将多个所有掩膜加权相乘得到可视化结果。有研究提出了基于机器学习的方法，将掩膜作为优化对象，通过定义限制掩膜的损失函数，不断迭代从而找到最优的掩膜。在此基础上，R.Fong等人\textsuperscript{\cite{fong2019understanding,fong2017interpretable}}提出通过限制搜索区域，重新设计损失函数，达到了很好的效果。基于扰动的可视化算法因为能够对模型预测结果进行直接观察，所以可以较为真实的反映模型决策机制。但是，这种类型算法往往需要多次迭代，需要付出高额的计算成本

\section{论文研究的主要内容}
本文的主要研究内容是探究当前图像分类神经网络的预测结果的可解释性，具体而言是生成显著图来赋予输入图片中每个像素一个权值来探究输入图片中物体特征对图像分类神经网络的预测结果的重要程度。利用显著图可以直观呈现图像分类神经网络对目标类别的判别依据，为用户提供可视化的解释。  

本文是基于图像分类神经网络的显著图解释研究，并侧重于提升生成显著图的分辨率，提高显著图对重要特征的定位能力。具体而言本文提出了两种创新方法。

（1）本研究提出了一种新的方法，可以针对基于卷积神经网络的图像分类模型的决策结果生成高分辨率的显著图。该方法采用多尺度放大原始输入图像，并将其输入到图像分类模型中。通过从最后一层卷积层获取不同分辨率的特征图集合，并对目标类别的分数进行反向传播，得到最后一层卷积层特征图对应的梯度矩阵。接着，将不同分辨率的特征图集合和梯度矩阵集合融合为原始输入图像的分辨率，并进行加权相加以生成掩膜。将所有掩膜分别应用于扰动原始输入图像，并输入到图像分类模型中，以获取目标类别的概率分数作为掩膜的权重。最后，对所有掩膜和其对应的概率分数进行加权相乘，生成最终的显著图。在ILSVRC 2012数据集\textsuperscript{\cite{ILSVRC}}、PASCAL VOC数据集\textsuperscript{\cite{pascal}}和COCO2014数据集\textsuperscript{\cite{coco}}上进行了定性和定量实验，结果显示该方法生成的显著图具有更高的分辨率，能够更准确地揭示图像分类神经网络在输入图像中的决策依据，提供更直观的可视化解释结果。

（2）本研究提出了一种通用的显著图增强方法，旨在解决当前显著图解释方法生成低分辨率显著图的问题。该方法适用于多种显著图解释方法，可以在不改变原有方法内部计算流程的情况下，达到提高显著图分辨率，增强显著图对特征的定位能力的效果。该方法通过使用固定尺寸的滑动窗口对输入图像的局部区域进行上采样，并将结果输入到选定的显著图解释方法中，得到针对特定类别的显著图和概率分数。随后，将显著图下采样到输入图像对应位置的窗口中，并乘以概率分数，生成具备更多细节高分辨率的显著图。该方法在不同显著图解释方法上的应用结果表明，无论是通过量化指标还是直观评测，都显示出显著图质量得到明显提升。

\section{论文组织结构} 
本文一共分为六个章节，其组织结构如下：

第1章，绪论。本章介绍了当前深度学习可解释性的研究背景并引出本文的基于图像分类神经网络的显著图解释研究内容；然后介绍国内外当前研究现状并重点介绍了三类显著图解释方法和其发展历程，接着围绕当前显著图解释方法的存在缺陷引出本文的重点研究内容即提高显著图分辨率，增强显著图对重要特征的定位能力，提供高质量的可视化解释。

第2章，相关理论介绍。该章节对本文使用的两种图像分类神经网络架构：卷积神经网络和Transformer架构进行了简要介绍；然后重点介绍了当前几种著名的显著图解释方法的背后的原理和其计算流程。

第3章，基于输入图片多尺寸放大的卷积神经网络显著图解释方法。该章节介绍本文为了解决当前针对卷积神经网络显著图解释方法生成的显著图分辨率较低，特征定位模糊的问题，提出了一种能生成高分辨率，特征定位更为准确的显著图解释方法并详细介绍了算法流程。然后设计实验通过在多个数据集上进行不同维度的实验评价，综合多个评价指标验证了提出方法的有效性。

第4章，基于二维滑动窗口的图像分类神经网络显著图增强方法。该章节针对当前显著图解释方法普遍存在的生成显著图分辨率低的问题，提出了一种简单易于实施的通用的显著图解释方法增强方法并详解介绍了其中的算法流程包括二维滑动窗口的算法过程。然后本章设计了实验，通过将提出的增强方法应用在多种显著图解释方法上，并选用了不同架构的图像分类神经网络和多种数据集进行多个维度的测试，分析测试结果显示该增强方法具有明显提升效果。

第5章，显著图解释方法对比评测系统。本章针对当前存在的多种显著图解释方法实施算法复杂对比困难的情况开发了一个显著图解释方法对比评测系统。本章依次介绍了系统需求分析、开发环境及相关开发技术、系统设计和系统展示。

第6章，总结和展望。本章对本文的所有研究内容和创新点进行了总结，并对当前深度学习可解释性领域和显著图解释方法进行了分析展望，说明未来可能存在的研究进展和突破。



\clearpage