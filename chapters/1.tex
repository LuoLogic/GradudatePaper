


\chapter{绪论}
\thispagestyle{others}
\pagestyle{others}
\xiaosi

\section{研究背景及意义}
当今的社会正处于智能化趋势的浪潮中，由深度学习理论所衍生的相关技术被广泛的应用在人们所熟知的各个领域。在学术界和工业界的互相融合促进下，深度学习算法不断推陈出新，深度神经网络也不断进化并发展出适用与文字、图像、视频等信息介质的自动化识别和信息提取的高效的深度神经网络模型，许多相关领域深度神经网络模型已经是现代社会正常运转的不可或缺的一部分。

但是，当前主流的深度神经网络并不具备良好的可解释性，即便这些深度神经网络在各种测试任务下展现出了很高的准确率。例如在图像分类应用中，将待分类的图片送入训练好的深度神经网络会得到不同类别物体的置信概率分数，即便某一类别的置信概率分数是99.99\%，我们也无法得到深度神经网络做出这一决策的依据，即该深度神经网络的输出结果并不具备可解释性。并且随着应用场景的复杂化多样化，深度神经网络结构日趋复杂，参数数量日趋庞大，这使得深度神经网络的“黑盒”特点变得更为突出。这种难以解释的黑盒特性使得深度神经网络在可靠性要求极高的领域，诸如医疗影像、自动驾驶、航空航天等领域的应用就受到限制。

除此之外，上述的黑盒特性也会成为当前的深度神经网络的研究过程当中的“拦路虎”。研究者往往是将训练好的深度神经网络在既有的数据集上根据各种外部的量化指标评价训练效果，但是在实际应用过程当中，模型可能会对现实世界中某些特殊的数据或者人为恶意伪造的数据给出异常的结果。如果训练的深度神经网络不能对这些意外数据有着良好的鲁棒性，那么该神经网络的就不能得到人们的信任。因此这使得相关研究者必须从更加全面的角度判断深度神经网络的性能表现并且解释其训练的神经网络的结果输出，而不只是依赖单一的评价指标来判断神经网络的性能表现。

因此，从深度神经网络实际应用和可靠性的角度出发，都需要有适用于深度神经网络黑盒特性的可解释方法，所以许多研究人员将目光转移到深度神经网络的可解释性上，他们或试图从既有的深度神经网络内外寻找其的决策依据，或试图从原理上构建可解释的深度神经网络，这两类研究路径分别就是后解释的人工智能和自解释的人工智能。

通过利用针对深度神经网络的可解释方法，不仅可以使得研究者和用户知道深度神经网络的决策依据，理解其中的决策机理，增强人对神经网络输出结果的信任程度，还可以使得研究者对神经网络的可靠性进行针对性的验证和测试。加强对深度神经网络可解释方法的研究有助于研究人员用更加全面且灵活双向的方式和神经网络进行交互，能做到“知其然并知其所以然”。可解释性的研究赋予了深度神经网络更多的可能性并加强了其可靠性。

本文主要聚焦于图像分类神经网络的可解释性研究，更加具体的是利用图像即显著图的方式来提供图像分类神经网络的可解释性，根据显著图中给出权值数据来解释图像分类神经网络的输出结果并判断其内部决策过程是否合理可靠。同时本文的研究侧重点在于不改变神经网络内部参数，仅利用既有训练好的深度神经网络模型来进行可解释性研究。既有的图像分类神经网络模型在工业界和学术界已经得到了大量的应用，并且拥有很好的性能表现，因此在不改变模型的前提下，本文的研究可以提供简单明了能直接使用的可解释方法，对已经训练完成模型的输出结果进行显著图分析解释，将单一的，不可解释的输出结果转变为直观明了的，可解释的图像结果，提高了模型的可靠性和可信任性。此外，显著图给出的分析结果可以帮助研究者有效分析模型是否学习到正确的特征，提供神经网络决策的关键依据。





\section{国内外研究现状}
\subsection{基于反向传播的显著图解释技术}
利用反向传播的机制来实现可视化解释是较早的一些工作所采用的方法。Zeiler等人\textsuperscript{\cite{zeiler2014visualizing}}提出了一种基于反卷积的可视化方法，反卷积将特征值逆映射回了输入图片的像素空间，借此说明图片中的哪些像素参与激活了该特征值。在这项工作的基础上，导向反向传播方法\textsuperscript{\cite{springenberg2014striving}}提出在反向传播时通过抑制输入和梯度小于0的值，从而突出可视化目标的重要特征。DeepLIFT\textsuperscript{\cite{shrikumar2017learning}}，LRP(Layer-wise Relevance Propagation)\textsuperscript{\cite{binder2016layer}}方法通过修改反向传播的规则，将输出层的贡献逐渐向下分配直到输入层，以此来获得输入图片中每个像素对输出相关性分数。Simonyan等人\textsuperscript{\cite{simonyan2014deep}}提出使用输入的梯度作为可视化解释的一种手段，这种方法认为输入的某些像素对网络的预测结果起到了主要作用，它直接计算网络输出的特定类别分数对输入的梯度，但是输入的梯度中包含明显的噪声，导致显著图可视化十分模糊。SmoothGrad方法\textsuperscript{\cite{smilkov2017smoothgrad}}和VarGrad方法\textsuperscript{\cite{adebayo2018local}}的原理都是共同的，它们向输入图片中多次添加噪声生成一组包含噪声的图片，通过平均化结果使得生成的显著性图更加平滑。为了解决梯度饱和问题，Sundararajan等人提出了一种积分梯度方法\textsuperscript{\cite{sundararajan2017axiomatic}}，该方法结合了直接计算梯度和基于反向传播的归因技术DeepLIFT和LRP的分而治之的设计思想，满足敏感性和实现不变性的公理。这些研究虽然有坚实的理论基础，但是它们的可视化结果对于人类来说不容易理解，而且噪声较多。此外这些方法中许多是和具体类别不相干的，无法对指定类别给出显著图可视化解释结果。另外有研究\textsuperscript{\cite{adebayo2018sanity }}指出其中有些方法的可靠性是值得怀疑的，它们对深度神经网络的参数不敏感，即使网络没有经过训练也能得到相似的结果。

\subsection{基于类激活映射的显著图解释技术}
基于类激活映射的方法是目前被大量研究和应用的一种流行的方法。这类方法利用了卷积神经网络中靠近输出端的信息，这些信息中包含着和预测结果相关的丰富的类别信息，所以这类方法能够给出类别相关的显著图可视化解释。CAM\textsuperscript{\cite{zhou2016learning}}方法首先提出了将卷积神经网络的最后一层全局平均池化后得到权重和该层提取的特征图线性相乘后累加  从而生成显著图。Grad-CAM\textsuperscript{\cite{selvaraju2017grad}}对CAM方法进行了改进，无需修改网络结构，利用反向传播的梯度取均值作为权重。Grad-CAM++\textsuperscript{\cite{chattopadhay2018grad}}进一步改进了Grad-CAM,它对不同像素的梯度进行加权，生成的显著图中能将同一类别物体出现多次的情况给较好的展示出来；XGrad-CAM\textsuperscript{\cite{fu2020axiom}}通过分析敏感性和实现不变性公理采用了另一种加权方法来获得特征图的权重，它引入了特征图中的像素权重为对应梯度进一步加权。为了减少噪声的影响，Smooth Grad-CAM++\textsuperscript{\cite{omeiza2019smooth}}，SS-CAM\textsuperscript{\cite{wang2020ss}}也采用了SmoothGrad\textsuperscript{\cite{smilkov2017smoothgrad}2}和VarGrad\textsuperscript{\cite{adebayo2018}2}中的向输入图片中多次添加噪声的措施。Score-CAM\textsuperscript{\cite{wang2020s}}和Ablation-CAM\textsuperscript{\cite{ramaswamy2020ablation}}没有使用反向传播中的梯度作为特征图的权重，它们将前向传播中从最后一层卷积层获得的特征图作为掩膜来扰动输入图片，利用网络输出值或者下降值作为特征图的权重，这种方法有效避免了使用梯度而产生的噪声，取得了良好的效果。Relevance-CAM\textsuperscript{\cite{lee2021relevance}}将卷积神经网络中提取的特征图再分别输入到卷积神经网络中，对每张特征图的预测结果进行层间相关性传播（Layer-wise Relevance Propagation），得到对应的相关性图，将相关性图全局平均池化后作为特征的权重，该方法有效解决了之前的CAM方法对卷积神经网络中间层可视化解释不足的问题。CAMERAS\textsuperscript{\cite{jalwana2021cameras}}提出了将输入图片进行多尺度放大再输入到网络当中，将提取到的不同分辨率的特征图和梯度全部放大到和原图分辨率一样。然而，基于类激活映射的方法只针对卷积神经网络进行设计，无法便捷的迁移到其他网络当中，比如基于Transformer的图片分类神经网络模型。


\subsection{基于扰动的显著图解释方法}
基于扰动的显著图可视化方法最突出的优点便是这种方法基本只关心网络的输入和输出，可以在不同结构网络轻松的应用，即使这些网络中的细节千差万别。Zeiler等人\textsuperscript{\cite{zeiler2014visualizing}2}使用一个固定形状的方块来对输入图片进行扰动，观察输出变化从而找到对模型而已输入图片中最重要的部分。SHAP方法\textsuperscript{\cite{lundberg2017unified}}使用了Shapley值，计算不同输入像素在是否存在的情况下对网络预测结果的影响，公平分配贡献度给这些像素点。LIME\textsuperscript{\cite{ribeiro2016should}}方法通过在较小的范围内扰动输入图片，得到输出结果，利用输入数据和输出数据重新训练一个可解释的代理模型去逼近黑盒模型在局部的决策边界，从而获得不同特征的重要性。RISE\textsuperscript{\cite{petsiuk2018rise}}利用蒙特卡洛方法随机生成的数量巨大的掩膜来扰动输入图片，将模型对特定类别输出概率作为的权重，再将多个所有掩膜加权相乘得到可视化结果。有研究提出了基于机器学习的方法，将掩膜作为优化对象，通过定义限制掩膜的损失函数，不断迭代从而找到最优的掩膜。在此基础上，Fong等人\textsuperscript{\cite{fong2019understanding}}提出通过限制搜索区域，重新设计损失函数，达到了很好的效果。基于扰动的可视化算法因为能够对模型预测结果进行直接观察，所以可以较为真实的反映模型决策机制。但是，这种类型算法往往需要多次迭代，需要付出高额的计算成本

\section{论文研究的主要内容}
学位论文……

\section{论文组织结构}
本文……

\clearpage