
%参考文献



\begin{thebibliography}{200}
\wuhao %设置参考文献字体大小
\linespread{1}\selectfont
\setlength{\itemsep}{-1.4ex} %缩小条目间行距
\thispagestyle{others}
\pagestyle{others}

\makeatletter
\renewcommand\@biblabel[1]{[#1]\hfill} %序号左对齐
\makeatother
\setlength{\labelsep}{0cm}

\bibitem{language}
奚雪峰, 周国栋. 面向自然语言处理的深度学习研究[J]. 自动化学报, 2016,42(10): 1445-1465.

\bibitem{voice}
侯一民, 周慧琼, 王政一. 深度学习在语音识别中的研究进展综述[J]. 计算机应用研究, 2017, 34(8): 2241-2246.

\bibitem{image}
KRIZHEVSKY A, SUTSKEVER I, HINTON G E. Imagenet classification with deep convolutional neural networks[J]. Communications of the ACM, 2017, 60(6): 84-90

\bibitem{simonyan2014very}
SIMONYAN K. Very deep convolutional networks for large-scale image recognition[J/OL]. arXiv: arXiv preprint, 2015[2019-04-10].https://arxiv.org/pdf/1409.1556.pdf.

\bibitem{od1}
REDMON J, FARHADI A. YOLO9000: better, faster, stronger[C]. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Honolulu, USA, 2017: 7263-7271.

\bibitem{od2}
SHAOQING R ,KAIMING H ,ROSS G , et al.Faster r-cnn: towards real-time object detection with region proposal networks.[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2017,39(6): 1137-1149.

\bibitem{sg1}
CHEN L C, ZHU Y, PAPANDREOU G, et al. Encoder-decoder with atrous separable convolution for semantic image segmentation[C].Computer Vision - ECCV 2018: 15th European Conference, Munich, Germany, 2018: 833-851.

\bibitem{sg2}
LONG J, SHELHAMER E, DARRELL T. Fully convolutional networks for semantic segmentation[C]. IEEE Conference on Computer Vision and Pattern Recognition. Boston, USA.2015: 3431-3440.

\bibitem{machine}
RAHWAN I, CEBRIAN M, OBRADOVICH N, et al. Machine behaviour[J]. Nature,2019, 568(7753): 477-486.

\bibitem{arospace}
REBUFFI S A, FONG R, JI X, et al. There and back again: revisiting backpropagation saliency methods[C]. IEEE/CVF Conference on Computer Vision and Pattern Recognition. Seattle, USA. 2020: 8839-8848.

\bibitem{post}
MITROS J, MAC NAMEE B. A categorisation of post-Hoc explanations for predictive models[J/OL]. arXiv: arXiv preprint, 2019[2019-04-04]. https://arxiv.org/pdf/1904.02495.

\bibitem{post2}
RAS G, XIE N, VAN GERVEN M, et al. Explainable deep learning: A field guide for the uninitiated[J]. Journal of Artificial Intelligence Research, 2022, 73: 329-397.

\bibitem{Intrinsic1}
LEVY O, DAGAN I. Annotating relation inference in context via question answering[C]. Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, Berlin, Germany, 2016: 249-255.

\bibitem{Intrinsic2}
YOON J, JORDON J, VAN DER SCHAAR M. INVASE: instance-wise variable selection using neural networks[C]. International Conference on Learning Representations, New Orleans, Louisiana, United States, 2018: 4857-4881.

\bibitem{Intrinsic3}
KIM J, ROHRBACH A, DARRELL T, et al. Textual explanations for self-driving vehicles[C]. Proceedings of the European Conference on Computer Vision, Munich, Germany, 2018: 563-578.

\bibitem{Intrinsic4}
CHANG S, ZHANG Y, YU M, et al. A game theoretic approach to class-wise selective rationalization[C]. Proceedings of the 33rd International Conference on Neural Information Processing Systems, Vancouver, Canada, 2019: 10055-10065.

\bibitem{Intrinsic5}
ZHANG Q, WU Y N, ZHU S C. Interpretable convolutional neural networks[C]. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Salt Lake City, UT, USA, 2018: 8827-8836.
\bibitem{Intrinsic6}
ELSAYED G, KORNBLITH S, LE Q V. Saccader: improving accuracy of hard attention models for vision[C]. 33rd International Conference on Neural Information Processing Systems, Vancouver, Canada, 2019: 702-714.

\bibitem{zeiler2014visualizing}
ZEILER M D, FERGUS R. Visualizing and understanding convolutional networks[C]. Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, 2014: 818-833.

\bibitem{springenberg2014striving}
SPRINGENBERG J T, DOSOVITSKIY A, BROX T, et al. Striving for simplicity: the all convolutional net[J/OL]. arXiv: arXiv preprint, 2019[2015-04-13]. https://arxiv.org/pdf/1412.6806.

\bibitem{shrikumar2017learning}
SHRIKUMAR A, GREENSIDE P, KUNDAJE A. Learning important features through propagating activation differences[C]. 34th International Conference on Machine Learning: ICML 2017, Sydney, Australia, 2017: 3145-3153.

\bibitem{binder2016layer}
BINDER A, MONTAVON G, LAPUSCHKIN S, et al. Layer-wise relevance propagation for neural networks with local renormalization layers[C]. Artificial Neural Networks and Machine Learning–ICANN 2016: 25th International Conference on Artificial Neural Networks, Barcelona, Spain, 2016: 63-71.

\bibitem{simonyan2014deep}
SIMONYAN K, VEDALDI A, ZISSERMAN A. Deep inside convolutional networks: visualising image classification models and saliency maps[J/OL]. arXiv: arXiv preprint, 2014[2015-04-19]. https://arxiv.org/pdf/1312.6034.

\bibitem{smilkov2017smoothgrad}
SMILKOV D, THORAT N, KIM B, ET al. Smoothgrad: removing noise by adding noise[J/OL]. arXiv: arXiv preprint, 2014[2015-07-12]. https://arxiv.org/pdf/1706.03825.

\bibitem{adebayo2018local}
ADEBAYO J, GILMER J, GOODFELLOW I, et al. Local explanation methods for deep neural networks lack sensitivity to parameter values[J/OL]. arXiv: arXiv preprint, 2018[2018-10-08]. https://arxiv.org/pdf/1810.03307.

\bibitem{sundararajan2017axiomatic}
SUNDARARAJAN M, TALY A, YAN Q. Axiomatic attribution for deep networks[C]. 34th International Conference on Machine Learning: ICML 2017, Sydney, Australia, 2018: 5109-5118.

\bibitem{adebayo2018sanity}
ADEBAYO J, GILMER J, MUELLY M, et al.  Sanity checks for saliency maps[C]. Proceedings of the 32nd International Conference on Neural Information Processing Systems, Montréal, Canada, 2018: 9525-9536.

\bibitem{zhou2016learning}
ZHOU B, KHOSLA A, LAPEDRIZA A, et al. Learning deep features for discriminative localization[C]. IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, USA, 2016: 2921-2929.

\bibitem{selvaraju2017grad}
SELVARAJU R R, COGSWELL M, DAS A, et al. Grad-CAM: visual explanations from deep networks via gradient-based localization[C]. IEEE International Conference on Computer Vision, Venice, Italy, 2017: 618-626.

\bibitem{chattopadhay2018grad}
CHATTOPADHAY A, SARKAR A, HOWLADER P, et al. Grad-CAM++: generalized gradient-based visual explanations for deep convolutional networks[C]. IEEE Winter Conference on Applications of Computer Vision, Lake Tahoe, USA, 2018: 839-847.

\bibitem{fu2020axiom}
FU R, HU Q, DONG X, et al. Axiom-based grad-cam: towards accurate visualization and explanation of cnns[C]. The 31st British Machine Vision Virtual Conference, UK, 2020: 1-18.

\bibitem{omeiza2019smooth}
OMEIZA D, SPEAKMAN S, CINTAS C, et al. Smooth Grad-CAM++: an enhanced inference level visualization technique for deep convolutional neural network models[J/OL]. arXiv: arXiv preprint, 2019[2019-8-03]. https://arxiv.org/pdf/1908.01224

\bibitem{wang2020ss}
WANG H, NAIDU R, MICHAEL J, et al. SS-CAM: smoothed score-CAM for sharper visual feature localization[J/OL]. arXiv: arXiv preprint, 2019[2020-11-12]. https://arxiv.org/pdf/2006.14255

\bibitem{ramaswamy2020ablation}
RAMASWAMY H G. Ablation-CAM: visual explanations for deep convolutional network via gradient-free localization[C]. IEEE Winter Conference on Applications of Computer Vision, Snowmass Village, USA, 2020: 972-980.

\bibitem{wang2020score}
WANG H, WANG Z, DU M, et al. Score-CAM: score-weighted visual explanations for convolutional neural networks[C]. IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops Seattle, USA, 2020: 111-119.

\bibitem{lee2021relevance}
LEE J R, KIM S, PARK I, et al. Relevance-cam: your model already knows where to look[C]. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Nashville, USA, 2021: 14944-14953.

\bibitem{jalwana2021cameras}
JALWANA M A A K, AKHTAR N, BENNAMOUN M, et al. CAMERAS: enhanced resolution and sanity preserving class activation mapping for image saliency[C]. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Nashville, USA, 2021: 16327-16336.

\bibitem{lundberg2017unified}
LUNDBERG S M, LEE S I. A unified approach to interpreting model predictions[C]. Proceedings of the 31st International Conference on Neural Information Processing Systems, Long Beach, USA, 2017: 4768–4777.

\bibitem{ribeiro2016should}
RIBEIRO M T, SINGH S, GUESTRIN C. " Why should i trust you?" explaining the predictions of any classifier[C]. Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, San Francisco, USA, 2016: 1135-1144.

\bibitem{petsiuk2018rise}
PETSIUK V, DAS A, SAENKO K. RISE: randomized input sampling for explanation of black-box models[C]. British Machine Vision Conference, Newcastle, UK, 2018: 151-159

\bibitem{fong2019understanding}
FONG R, PATRICK M, VEDALDI A. Understanding deep networks via extremal perturbations and smooth masks[C]. IEEE/CVF International Conference on Computer Vision, Seoul, Korea, 2019:2950-2958.

\bibitem{fong2017interpretable}
FONG R C, VEDALDI A. Interpretable explanations of black boxes by meaningful perturbation[C]. IEEE International Conference on Computer Vision, Venice, Italy, 2017: 3449-3457.

\bibitem{ILSVRC}
RUSSAKOVSKY O, DENG J, SU H, et al. Imagenet large scale visual recognition challenge[J]. International Journal of Computer Vision, 2015, 115: 211-252.

\bibitem{pascal}
EVERINGHAM M, VAN GOOL L, WILLIAMS C K I, et al. The pascal visual object classes (voc) challenge[J]. International Journal of Computer Vision, 2010, 88: 303-338.

\bibitem{coco}
LIN T Y, MAIRE M, BELONGIE S, et al. Microsoft coco: common objects in context[C]. Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, 2014: 740-755.

\bibitem{bengio2013representation}
BENGIO Y, COURVILLE A, VINCENT P. Representation learning: a review and new perspectives[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2013, 35(8): 1798-1828.

\bibitem{vaswani2017attenion}
VASWANI A, SHAZEER N, PARMAR N, et al. Attention is all you need[C]. Proceedings of the 31st International Conference on Neural Information Processing Systems, Long Beach, USA, 2017: 6000–6010

\bibitem{kenton2019bert}
KENTON J D M W C, TOUTANOVA L K. BERT: pre-training of deep bidirectional transformers for language understanding[C]. Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL HLT 2019), Minneapolis, USA, 2019:4171-4186.

\bibitem{floridi2020gpt}
FLORIDI L, CHIRIATTI M. GPT-3: its nature, scope, limits, and consequences[J]. Minds and Machines, 2020, 30: 681-694.

\bibitem{dosovitskiy2020image}
DOSOVITSKIY A, BEYER L, KOLESNIKOV A, et al. An image is worth 16x16 words: Transformers for image recognition at scale[J]. arXiv: arXiv preprint, 2021[2019-06-03]. https://arxiv.org/pdf/2010.11929.pdf

\bibitem{chefer2021transformer}
CHEFER H, GUR S, WOLF L. Transformer interpretability beyond attention visualization[C]. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Nashville, USA, 2021: 782-791.

\bibitem{mahendran2016visualizing}
MAHENDRAN A, VEDALDI A. Visualizing deep convolutional neural networks using natural pre-images[J]. International Journal of Computer Vision, 2016, 120: 233-255.

\bibitem{simonyan2014visualising}
SIMONYAN K, VEDALDI A, ZISSERMAN A. Deep inside convolutional networks: visualising image classification models and saliency maps[C]. Proceedings of the International Conference on Learning Representations, Banff, Canada, 2014: 1-8


\bibitem{montavon2017explaining}
MONTAVON G, LAPUSCHKIN S, BINDER A, et al. Explaining nonlinear classification decisions with deep taylor decomposition[J]. Pattern Recognition, 2017, 65211-222.


\bibitem{zhang2021novel}
ZHANG Q, RAO L, YANG Y. A novel visual interpretability for deep neural networks by optimizing activation maps with perturbation[C]. 35th AAAI Conference on Artificial Intelligence, Online, 2021:3377-3384.

\bibitem{kapishnikov2019xrai}
KAPISHNIKOV A, BOLUKBASI T, VIÉGAS F, et al.XRAI: better attributions through regions[C]. IEEE/CVF International Conference on Computer Vision, Seoul, Korea, 2019:4947-4956.

\bibitem{dabkowski2017real}
DABKOWSKI P, GAL Y. Real time image saliency for black box classifiers[C]. Proceedings of the 31st International Conference on Neural Information Processing Systems, Long Beach, USA, 2017: 6970–6979.

\bibitem{zhang2018top}
ZHANG J, BARGAL S A, LIN Z, et al. Top-down neural attention by excitation backprop[J]. International Journal of Computer Vision, 2018, 126(10): 1084-1102.

\bibitem{guillaumin2014imagenet}
GUILLAUMIN M, KÜTTEL D, FERRARI V. Imagenet auto-annotation with segmentation propagation[J]. International Journal of Computer Vision, 2014, 110: 328-348.



\end{thebibliography}

%\bibliographystyle{unsrt}


% 此参考文献名为ref.bib文件

%\bibliography{ref}
%\thispagestyle{others}





\clearpage