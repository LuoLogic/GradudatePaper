
%参考文献



\begin{thebibliography}{200}
\wuhao %设置参考文献字体大小
\linespread{1}\selectfont
\setlength{\itemsep}{-1.4ex} %缩小条目间行距
\thispagestyle{others}
\pagestyle{others}

\makeatletter
\renewcommand\@biblabel[1]{[#1]\hfill} %序号左对齐
\makeatother
\setlength{\labelsep}{0cm}

\bibitem{language}
奚雪峰, 周国栋. 面向自然语言处理的深度学习研究[J]. 自动化学报, 2016,42(10): 1445-1465.

\bibitem{voice}
侯一民, 周慧琼, 王政一. 深度学习在语音识别中的研究进展综述[J]. 计算机应用研究, 2017, 34(8): 2241-2246.

\bibitem{image}
Krizhevsky A, Sutskever I, Hinton G E. Imagenet Classification With Deep Convolutional Neural Networks[J]. Communications of the ACM, 2017, 60(6):84-90

\bibitem{simonyan2014very}
Simonyan K. Very deep convolutional networks for large‐scale image recognition. 3 rd Int Conf learn represent ICLR 2015‐Conf track proc[J]. Published online, 2015: 1.

\bibitem{od1}
Redmon J, Farhadi A. YOLO9000: better, faster, stronger[C] // Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 7263-7271.
\bibitem{od2}
Ren S, He K, Girshick R, et al. Faster r-cnn: Towards real-time object detection with region proposal networks[J]. Advances in neural information processing systems, 2015, 28.

\bibitem{sg1}
Chen L C, Zhu Y, Papandreou G, et al. Encoder-decoder with atrous separable convolution for semantic image segmentation[C] // Proceedings of the European conference on computer vision (ECCV). 2018: 801-818.
\bibitem{sg2}
Long J, Shelhamer E, Darrell T. Fully convolutional networks for semantic segmentation[C] // Proceedings of the IEEE conference on computer vision and pattern recognition. 2015: 3431-3440.

\bibitem{machine}
Rahwan I, Cebrian M, Obradovich N, et al. Machine Behaviour[J]. Nature,2019, 568(7753): 477-486.

\bibitem{arospace}
Rebuffi S A, Fong R, Ji X, et al. There and back again: Revisiting backpropagation saliency methods[C] // Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 8839-8848.

\bibitem{post}
Mitros J, Mac Namee B. A Categorisation of Post-Hoc Explanations for Predictive Models[J]. Arxiv Preprint Arxiv:1904.02495, 2019.

\bibitem{post2}
Ras G, Xie N, Van Gerven M, et al. Explainable Deep Learning: A Field Guide for the Uninitiated[J]. Journal of Artificial Intelligence Research, 2022, 73:329-397.

\bibitem{Intrinsic1}
Levy O, Dagan I. Annotating Relation Inference In Context Via Question Answering[C] // Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), Berlin, Germany, 2016:249-255.

\bibitem{Intrinsic2}
Yoon J, Jordon J, Van Der Schaar M. INVASE: Instance-Wise Variable Selection Using Neural Networks[C] // International Conference on Learning Representations, New Orleans, Louisiana, United States, 2018: 4857-4881.

\bibitem{Intrinsic3}
Kim J, Rohrbach A, Darrell T, et al. Textual Explanations for Self-Driving Vehicles[C] // Proceedings of the European Conference on Computer Vision (ECCV), Munich, GERMANY, 2018: 563-578.

\bibitem{Intrinsic4}
Chang S, Zhang Y, Yu M, et al. A Game Theoretic Approach to Class-Wise Selective Rationalization[J]. Advances in Neural Information Processing Systems, 2019, 32: 10055-10065.
\bibitem{Intrinsic5}
Zhang Q, Wu Y N, Zhu S C. Interpretable Convolutional Neural Networks[C] // Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Salt Lake City, UT, USA, 2018: 8827-8836.
\bibitem{Intrinsic6}
Elsayed G, Kornblith S, Le Q V. Saccader: Improving Accuracy of Hard Attention Models for Vision[J]. Advances in Neural Information Processing Systems, 2019, 32: 700-712.

\bibitem{zeiler2014visualizing}
Zeiler M D, Fergus R. Visualizing and understanding convolutional networks[C] // Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part I 13. Springer International Publishing, 2014: 818-833.

\bibitem{springenberg2014striving}
Springenberg J T, Dosovitskiy A, Brox T, et al. Striving for simplicity: The all convolutional net[J/OL]. arXiv preprint arXiv:1412.6806, 2014.

\bibitem{shrikumar2017learning}
Shrikumar A, Greenside P, Kundaje A. Learning important features through propagating activation differences[C] // International conference on machine learning. PMLR, 2017: 3145-3153.

\bibitem{binder2016layer}
Binder A, Montavon G, Lapuschkin S, et al. Layer-wise relevance propagation for neural networks with local renormalization layers[C] // Artificial Neural Networks and Machine Learning–ICANN 2016: 25th International Conference on Artificial Neural Networks, Barcelona, Spain, September 6-9, 2016, Proceedings, Part II 25. Springer International Publishing, 2016: 63-71.

\bibitem{simonyan2014deep}
Simonyan K, Vedaldi A, Zisserman A. Deep inside convolutional networks: Visualising image classification models and saliency maps[J/OL]. arXiv preprint arXiv:1312.6034, 2013.

\bibitem{smilkov2017smoothgrad}
Smilkov D, Thorat N, Kim B, et al. Smoothgrad: removing noise by adding noise[J/OL]. arXiv preprint arXiv:1706.03825, 2017.

\bibitem{adebayo2018local}
Adebayo J, Gilmer J, Goodfellow I, et al. Local explanation methods for deep neural networks lack sensitivity to parameter values[J]. arXiv preprint arXiv:1810.03307, 2018.

\bibitem{sundararajan2017axiomatic}
Sundararajan M, Taly A, Yan Q. Axiomatic attribution for deep networks[C] // International conference on machine learning. PMLR, 2017: 3319-3328.

\bibitem{adebayo2018sanity}
Adebayo J, Gilmer J, Muelly M, et al. Sanity checks for saliency maps[J]. Advances in neural information processing systems, 2018, 31.

\bibitem{zhou2016learning}
Zhou B, Khosla A, Lapedriza A, et al. Learning deep features for discriminative localization[C] // Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 2921-2929.

\bibitem{selvaraju2017grad}
Selvaraju R R, Cogswell M, Das A, et al. Grad-cam: Visual explanations from deep networks via gradient-based localization[C] // Proceedings of the IEEE international conference on computer vision. 2017: 618-626.

\bibitem{chattopadhay2018grad}
Chattopadhay A, Sarkar A, Howlader P, et al. Grad-cam++: Generalized gradient-based visual explanations for deep convolutional networks[C] // 2018 IEEE winter conference on applications of computer vision (WACV). IEEE, 2018: 839-847.

\bibitem{fu2020axiom}
Fu R, Hu Q, Dong X, et al. Axiom-based grad-cam: Towards accurate visualization and explanation of cnns[J]. arXiv preprint arXiv:2008.02312, 2020.

\bibitem{omeiza2019smooth}
Omeiza D, Speakman S, Cintas C, et al. Smooth grad-cam++: An enhanced inference level visualization technique for deep convolutional neural network models[J]. arXiv preprint arXiv:1908.01224, 2019.

\bibitem{wang2020ss}
Wang H, Naidu R, Michael J, et al. SS-CAM: Smoothed Score-CAM for sharper visual feature localization[J]. arXiv preprint arXiv:2006.14255, 2020.

\bibitem{ramaswamy2020ablation}
Ramaswamy H G. Ablation-cam: Visual explanations for deep convolutional network via gradient-free localization[C] // proceedings of the IEEE/CVF winter conference on applications of computer vision. 2020: 983-991.

\bibitem{wang2020score}
Wang H, Wang Z, Du M, et al. Score-CAM: Score-weighted visual explanations for convolutional neural networks[C] // Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops. 2020: 24-25.

\bibitem{lee2021relevance}
Lee J R, Kim S, Park I, et al. Relevance-cam: Your model already knows where to look[C] // Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021: 14944-14953.

\bibitem{jalwana2021cameras}
Jalwana M A A K, Akhtar N, Bennamoun M, et al. CAMERAS: Enhanced resolution and sanity preserving class activation mapping for image saliency[C] // Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021: 16327-16336.

\bibitem{lundberg2017unified}
Lundberg S M, Lee S I. A unified approach to interpreting model predictions[J]. Advances in neural information processing systems, 2017, 30.

\bibitem{ribeiro2016should}
Ribeiro M T, Singh S, Guestrin C. " Why should i trust you?" Explaining the predictions of any classifier[C] // Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining. 2016: 1135-1144.

\bibitem{petsiuk2018rise}
Petsiuk V, Das A, Saenko K. Rise: Randomized input sampling for explanation of black-box models[J]. arXiv preprint arXiv:1806.07421, 2018.

\bibitem{fong2019understanding}
Fong R, Patrick M, Vedaldi A. Understanding deep networks via extremal perturbations and smooth masks[C] // Proceedings of the IEEE/CVF international conference on computer vision. 2019: 2950-2958.
\bibitem{fong2017interpretable}
Fong R C, Vedaldi A. Interpretable explanations of black boxes by meaningful perturbation[C] // Proceedings of the IEEE international conference on computer vision. 2017: 3429-3437.

\bibitem{ILSVRC}
Russakovsky O, Deng J, Su H, et al. Imagenet large scale visual recognition challenge[J]. International journal of computer vision, 2015, 115: 211-252.
\bibitem{pascal}
Everingham M, Van Gool L, Williams C K I, et al. The pascal visual object classes (voc) challenge[J]. International journal of computer vision, 2010, 88: 303-338.
\bibitem{coco}
Lin T Y, Maire M, Belongie S, et al. Microsoft coco: Common objects in context[C] // Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13. Springer International Publishing, 2014: 740-755.

\bibitem{bengio2013representation}
Bengio Y, Courville A, Vincent P. Representation learning: A review and new perspectives[J]. IEEE transactions on pattern analysis and machine intelligence, 2013, 35(8): 1798-1828.

\bibitem{vaswani2017attenion}
Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[J]. Advances in neural information processing systems, 2017, 30.
\bibitem{kenton2019bert}
Kenton J D M W C, Toutanova L K. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding[C]//Proceedings of NAACL-HLT. 2019: 4171-4186.
\bibitem{floridi2020gpt}
Floridi L, Chiriatti M. GPT-3: Its nature, scope, limits, and consequences[J]. Minds and Machines, 2020, 30: 681-694.

\bibitem{dosovitskiy2020image}
Dosovitskiy A, Beyer L, Kolesnikov A, et al. An image is worth 16x16 words: Transformers for image recognition at scale[J]. arXiv preprint arXiv:2010.11929, 2020.

\bibitem{chefer2021transformer}
Chefer H, Gur S, Wolf L. Transformer interpretability beyond attention visualization[C] // Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021: 782-791.

\bibitem{mahendran2016visualizing}
Mahendran A, Vedaldi A. Visualizing deep convolutional neural networks using natural pre-images[J]. International Journal of Computer Vision, 2016, 120: 233-255.

\bibitem{simonyan2014visualising}
Simonyan K, Vedaldi A, Zisserman A. Visualising image classification models and saliency maps[J]. Deep Inside Convolutional Networks, 2014, 2.



\bibitem{montavon2017explaining}
Montavon G, Lapuschkin S, Binder A, et al. Explaining nonlinear classification decisions with deep taylor decomposition[J]. Pattern recognition, 2017, 65: 211-222.


\bibitem{zhang2021novel}
Zhang Q, Rao L, Yang Y. A novel visual interpretability for deep neural networks by optimizing activation maps with perturbation[C] // Proceedings of the AAAI Conference on Artificial Intelligence. 2021, 35(4): 3377-3384.

\bibitem{kapishnikov2019xrai}
Kapishnikov A, Bolukbasi T, Viégas F, et al. Xrai: Better attributions through regions[C] // Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019: 4948-4957.

\bibitem{dabkowski2017real}
Dabkowski P, Gal Y. Real time image saliency for black box classifiers[J]. Advances in neural information processing systems, 2017, 30.

\bibitem{zhang2018top}
Zhang J, Bargal S A, Lin Z, et al. Top-down neural attention by excitation backprop[J]. International Journal of Computer Vision, 2018, 126(10): 1084-1102.

\bibitem{guillaumin2014imagenet}
Guillaumin M, Küttel D, Ferrari V. Imagenet auto-annotation with segmentation propagation[J]. International Journal of Computer Vision, 2014, 110: 328-348.



\end{thebibliography}

%\bibliographystyle{unsrt}


% 此参考文献名为ref.bib文件

%\bibliography{ref}
%\thispagestyle{others}





\clearpage