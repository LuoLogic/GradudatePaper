

\chapter{基于输入图片多尺寸放大的卷积神经网络显著图解释方法}
\thispagestyle{others}
\pagestyle{others}
\xiaosi

\section{本章引言}



\section{问题描述和研究思路}
当前大多数基于类激活映射的显著图解释方法诸如CAM、Grad-CAM、Grad-CAM++、XGrad-CAM、Score-CAM等都存在一个共有问题，即最终生成的显著图的分辨率较低，这导致其只能在显著图中呈现一个模糊的解释效果，无法聚焦更加精细的特征。究其原因是这些基于类激活映射的方法一般提取的都是卷积神经网络的最后一层卷积层的输出作为特征图的来源，因该层包含较为丰富的类别特征信息，之后通过各种加权方法组合这些特征图来得到最终的显著图。因为卷积神经网络的结构特性，其最后一层卷积层的会输出多个通道的低分辨率特征图，因此无论如何组合这些特征图，最终得到的也只是一张分辨率和最后一层的卷积层输出的单一通道的特征图相当的初级显著图。当然一般情况下为了得到显著图和原始输入图片的特征对应关系，都会将这张原始输入图片进行分辨率层面的放大，使用的一般也是双线性插值算法，但这并不意味着原始显著图的有效信息的增多。

这里以VGG19这个基于卷积神经网络的图片分类模型举例，若输入图片的尺寸是3$\times$224$\times$224，其中3表示RGB颜色通道，224$\times$224表示图片分辨率，那么该模型最后一层卷积层的输出的特征图尺寸是512$\times$14$\times$14，其中数字512是通道数量，14$\times$14是特征图的分辨率。若我们使用双线性插值函数$\phi(s,H,W)$将原始显著图分辨率提升到224$\times$224,那么意味着我们插入了额外99.9\%的信息，而最终的显著图中的这些额外的像素信息并不能反映图片分类模型对原始输入图片的对应像素的感兴趣程度。即便有部分文献\textsuperscript{\cite{adebayo2018sanity}\cite{selvaraju2017grad}}试图改进这种插值函数，但是它们仍然引入了外部不相关的信息。

因此为了解决上述的问题，本文从特征图的提取这一关键点着手。考虑到若使用原始输入图片，那么卷积神经网络最后一层卷积层输出的单一特征图分辨率较低且包含的特征信息有限，因此本文提出基于输入图片多尺寸放大的卷积神经网络显著图解释方法，其核心就是将输入图片进行逐级多尺寸的双线性插值放大，例如将输入图片放大至334$\times$334、434$\times$434、534$\times$534等分辨率，这时能得到一组分辨率不同的输入图片，接着将这组输入图片分别送入基于卷积神经网络的图像分类模型中，再从其最后一层卷积层中分别提取特征图和梯度矩阵数据，基于卷积神经网络的采样特性，可以得到不同分辨率的特征图，更高分辨率的输入图片即对应更高分辨率的特征图，也即拥有更丰富的特征信息。因此若将这些特征图的信息进行融合即能得到分辨率更高的掩膜，再用这些更高分辨率的掩膜对原始输入图片进行扰动，即可得到相应的掩膜权重，用权重和高分辨率掩膜进行融合即能得到包含更多特征信息并且分辨率更高的显著图。
\section{基于输入图片多尺寸放大的卷积神经网络显著图解释方法}
本章提出基于基于输入图片多尺寸放大的卷积神经网络显著图解释方法来解决目前可视化算法结果分辨率低、无法给出更详细目标特征的问题。为了方便本章后文的描述，本文对该方法的简称是MSG-CAM（Multi-Scale inputs of Group-
CAM）。MSG-CAM的算法流程描述参见算法\ref{alg:1}，算法流程图参见图\ref{fig:msg_pipeline}。

\begin{figure}[h]
	\centering 
	\includegraphics[width=15cm]{fig/ch3/msg_pipeline.pdf}
	\bicaption[\xiaosi MSG-CAM算法流程示意图]{\wuhao MSG-CAM算法流程示意图}{\wuhao Pipeline of MSG-CAM}
	\label{fig:msg_pipeline}
\end{figure}

 \begin{algorithm}
	\caption{MSG-CAM}
	\label{alg:1}
	\renewcommand{\algorithmicrequire}{\textbf{输入:}}
	\renewcommand{\algorithmicensure}{\textbf{输出：}}
	\begin{algorithmic}[1]
		\REQUIRE 基于卷积神经网络的图像分类模型 $\mathcal{F} $ ,原始输入图片 $I_0 \in \mathbb{R}^{3\times H_0 \times W_0}$ ,上采样分辨率上限 $\zeta_{max}=(H_{max},W_{max})$ ,卷积层 $l$ ,最大迭代次数 $N$ ,类别 $c$ ,每个分组中掩膜数量$B$, 高斯模糊参数： $kernel\_size,sigma$, interpolation function $\varphi(.)$
		\ENSURE 显著图 $L_{MSG-CAM}\in \mathbb{R}^{3\times H_0 \times W_0}$
		\STATE 初始化 $L_{MSG-CAM} \leftarrow 0$ , $\mathcal{A}_0 \leftarrow 0$ , $\mathcal{G}_0 \leftarrow 0$ , $r \leftarrow 0$ , $t_{max}\leftarrow 0$ , $t_m\leftarrow 0$ , $kernel\_size=51,sigma=50$ ,基准图片 $I^{\prime}_0=guassian\_blur2d(I_0,kernel\_size,sigma)$ , $c=\mathcal{F}(I_0)$
		\WHILE{$t\leq N$}
		\STATE $t\leftarrow t+1$
		\STATE $\zeta_t\leftarrow \zeta_0 +\lfloor\frac{\zeta_{max}}{N}\rfloor(t-1)$
		\STATE $I_t\leftarrow \varphi(I_0,\zeta_t)$
		\IF{$\mathcal{F}(I_t) \rightarrow c$}
		\STATE $t_{max}\leftarrow t_{max}+1$
		\STATE $\overset{*}{\boldsymbol{A}_t}\leftarrow \mathcal{A}(I_t,l)$
		\STATE $\overset{*}{\boldsymbol{G}_t}\leftarrow \nabla\mathcal{J}(l,c)$
		\STATE $\mathcal{A}_t \leftarrow \mathcal{A}_{t-1}+\varphi(\overset{*}{\boldsymbol{A}_t},\zeta_0)$
		\STATE $\mathcal{G}_t \leftarrow \mathcal{G}_{t-1}+\varphi(\overset{*}{\boldsymbol{G}_t},\zeta_0)$
		\ENDIF
		\ENDWHILE
		\STATE $\overline{\bm{A}}\leftarrow \mathcal{A}_t/t_{max}$
		\STATE $\overline{\bm{G}}\leftarrow \mathcal{G}_t/t_{max}$
		\STATE $\overline{\bm{W}}\leftarrow \frac{1}{m\times n}\sum_{i=i}^{m}\sum_{j=1}^{n}\overline{\bm{G}}$
		\STATE $\boldsymbol{M}=\overline{\bm{A}}\cdot\overline{\bm{W}}$
		\STATE $K\leftarrow$ $\boldsymbol{M}$的通道数量;
		\STATE $g\leftarrow$ 每个分组中掩膜数量;
		\WHILE{$r<B$}
		\STATE 在组内生成单一的掩膜\\ $M_r=ReLU(\sum_{k=r\times g}^{(r+1)\times g-1}M^k)$
		\STATE 初始化 $M^{\prime}_r\leftarrow$ 将$M^r$进行去噪操作和归一化
		\STATE 用掩膜扰动原始输入图片\\$I_r=I_0\odot M^{\prime}_r+I^{\prime}_0\odot (1-M^{\prime}_r)$
		\STATE 计算每张掩膜的权重\\$\alpha_r=\mathcal{F}_c(I_r)-\mathcal{F}_c(I^{\prime}_0)$
		\STATE $L_{MSG-CAM}\leftarrow L_{MSG-CAM}+\alpha_r M_r$
		\STATE $r\leftarrow r+1$
		\ENDWHILE
		\RETURN $ReLU(L^c_{MSG-CAM})$
	\end{algorithmic}
\end{algorithm}
\subsection{生成高分辨率掩膜}
令原始输入图片为$I_0 \in \mathbb{R}^{3\times H_0 \times W_0}$，其中3表示RGB颜色通道数量，$H_0$和$W_0$分别原始输入图片的长和宽的像素个数，即原始输入图片的分辨率可以表示为$\zeta_0=(H_0,W_0)$。此外，我们用$\mathcal{F}$表示一个预训练好的基于卷积神经网络的图像分类模型，那么$\mathcal{F}_c(I_0)$则表示在输入图片为$I_0$的情况下，图像分类模型$\mathcal{F}$对于类别$c$的输出分数，值得注意的是，该分数是未经归一化指数函数（softmax函数）之前的分数。将原始输入图片送入图像分类模型$\mathcal{F}$，并从其最后一层卷积层$l$当中提取所有通道的特征图集合，该特征图集合表示为$\overset{*}{\boldsymbol{A}_0}$。通过获得的类别$c$的输出分数$\mathcal{F}_c(I_0)$，对该分数反向传播至最后一层卷积层特征图，则可以获得对应特征图关于类别$c$的梯度矩阵$\overset{*}{\boldsymbol{G}_0}$，计算公式如下：
\begin{equation}
\overset{*}{\boldsymbol{G}_0}=\frac{\partial \mathcal{F}_c(I_0)}{\partial \overset{*}{\boldsymbol{A}_0}}
\label{eq:g0}
\end{equation}
\ref{eq:g0}公式中，梯度矩阵集合$\overset{*}{\boldsymbol{G}_0}$一共有$K$个通道，通道数量和特征图集合$\overset{*}{\boldsymbol{A}_0}$一致，并且每个通道的梯度矩阵和特征图一一对应。 

接着将原始输入图片$I_0$经由双线性插值函数$\varphi(I_0,\zeta_t)$上采样至图片$I_t$，$t$表示第$t$次上采样结果，经由上采样后的$I_t$的分辨率可表示为$\zeta_t$，$\zeta_t$是由于原始输入图片的分辨率$\zeta_0$逐步递增得来的。为了控制计算的时间成本，考虑将$\zeta_{max}=(H_{max},W_{max})$作为上采样分辨率的上限，同时设$N$为最大迭代次数即上采样次数,那么第$t$次上采样得到的图片分辨率可由以下公式计算得出：
\begin{equation}
	\zeta_t=\zeta_0 +\lfloor\frac{\zeta_{max}}{N}\rfloor(t-1)
\end{equation}
在上述迭代过程当中，每次上采样获得的输入图片$I_t$的分辨率都不同，则将其输入到图像分类模型当中$\mathcal{F}$从最后一层卷积层提取的特征图集合$\overset{*}{\boldsymbol{A}_t}$的分辨率也不同，而且随着输入图片分辨率的提高，$\overset{*}{\boldsymbol{A}_t}$的分辨率也会相应提高，相对应的$\overset{*}{\boldsymbol{A}_t}$也能包含更多和类别$c$相关的细节特征信息。因此若将这些不同的分辨率的特征图集合进行融合则能够得到更多的特征信息。融合的方法可由以下的公式进行表示：
\begin{equation}
	\overline{\bm{A}}=\frac{1}{t_{max}}\sum_{t=0}^{t_{max}}\varphi(\overset{*}{\boldsymbol{A}_t},\zeta_0)
	\label{eq:Afuse}
\end{equation}
公式\ref{eq:Afuse}中，$t_{max}$表示最大的有效迭代次数，此处的“有效”指的是只有当图像分类模型的输出结果中概率分数最大的类别是$c$时才采用这次迭代的特征图集合$\overset{*}{\boldsymbol{A}_t}$，其他情况下均舍弃,因此不难得出$t_{max} \leq N$。同时$\varphi(\overset{*}{\boldsymbol{A}_t},\zeta_0)$表示将特征图集合$\overset{*}{\boldsymbol{A}_t}$中的每一个通道上的特征图均上采样至原始输入图片的分辨率$\zeta_0$，这样做的目的是方便将不同分辨率的特征图集合进行融合，也方便后续对原始输入图片进行扰动。

在每次迭代过程中，都会计算并提取保存图像分类模型关于类别c的输出分数$\mathcal{F}_c(I_t)$对于卷积层$l$的特征图集合$\overset{*}{\boldsymbol{A}_t}$的反向传播梯度$\overset{*}{\boldsymbol{G}_t}$，和特征图集合一样，将不同迭代过程中保存的梯度矩阵集合$\overset{*}{\boldsymbol{G}_t}$也进行融合，可以得到$\overline{\bm{A}}$分辨率尺寸一致梯度矩阵集合，融合的公式如下所示：
\begin{equation}
	\overline{\bm{G}}=\frac{1}{t_{max}}\sum_{t=0}^{t_{max}}\varphi(\overset{*}{\boldsymbol{G}_t},\zeta_0)
	\label{eq:Gfuse}
\end{equation}
同样，公式\ref{eq:Gfuse}中，同时$\varphi(\overset{*}{\boldsymbol{G}_t},\zeta_0)$表示将梯度矩阵集合$\overset{*}{\boldsymbol{G}_t}$中的每一个通道上的梯度矩阵均上采样至原始输入图片的分辨率$\zeta_0$。由于融合后的特征图集合中不仅包含类别$c$特征信息，也包含其他类别的特征信息，因此为了将类别$c$的特征信息进行凸显，其他类别的特征信息进行削弱，我们将利用融合后的梯度矩阵$\overline{\bm{G}}$,它在一定程度上反映了特征图上不同像素对类别分数$\mathcal{F}_c(I_0)$的敏感程度，或者说是重要程度\textsuperscript{\cite{selvaraju2017grad}3}。仿照Grad-CAM\textsuperscript{\cite{selvaraju2017grad}4}中的方法，我们将$\overline{\bm{G}}$中每张梯度矩阵进行全局平均池化，在每个通道上得到一个权重值，每个通道下的权重值都和$\overline{\bm{A}}$中的每个通道下的特征图一一对应，所有通道下权重值的集合的计算公式如下：
\begin{equation}
	\overline{\bm{W}}=\frac{1}{H_0\times W_0}\sum_{i=1}^{H_0}\sum_{j=1}^{W_0}\overline{\bm{G}}
	\label{eq:Wfuse}
\end{equation}
额外说明的是，融合后的特征图集合$\overline{\bm{A}}$的尺寸是$[1\times K \times H_0\times W_0]$，它对应的权重集合$\overline{\bm{W}}$的尺寸是$[1\times K \times 1\times 1]$，其中$K$就是第$l$层卷积层输出的特征图通道数量。

本小节的最后，初始掩膜集合$\boldsymbol{M}$可以通过以下公式计算得出：
\begin{equation}
	\boldsymbol{M}=	\overline{\bm{A}}\cdot\overline{\bm{W}}
\end{equation}
其中，运算符$\cdot$表示$\overline{\bm{A}}$中的每个通道下的特征图中的每个像素值都和$\overline{\boldsymbol{W}}$中对应每个权值相乘。

\subsection{掩膜优化}
$\boldsymbol{M}$是经由输入图片多尺寸上采样放大后从卷积层中提取的特征图和梯度矩阵结合而得到的，相比单一原始输入图片得到掩膜（比如Score-CAM\textsuperscript{\cite{wang2020score}2}），$\boldsymbol{M}$包含更丰富的类别特征信息，但是目前得到$\boldsymbol{M}$仍然不能直接当作掩膜来扰动输入图片，它还有两个明显的缺点。第一点是$\boldsymbol{M}$的尺寸是$[1\times K \times H_0\times W_0]$，其中$K$是卷积层$l$输出的特征图的通道数量，一般我们取最后一层卷积层作为$l$，因此$K$的值将会是数百至上千，显然这样掩膜的数量太多了，若像Score-CAM\textsuperscript{\cite{wang2020score}3}一样逐个将掩膜去扰动原始输入图片得到权值，那样将会极为耗时。第二个缺点$\boldsymbol{M}$就是使用全局平均池化后的梯度作为特征图的权重，由于ReLU函数的零梯度问题\textsuperscript{\cite{zhang2021novel}}，这意味着$\boldsymbol{M}$显然会有噪声。因此，为了使$\boldsymbol{M}$成为合格的掩膜，需要让它变得更加纯净。

对于第一个缺陷，本节的解决办法是将$\boldsymbol{M}$中的$K$张掩膜平分为$B$个组，分类依据是它们的相邻关系，接着将每个组内的所有掩膜相加合并为一个掩膜，这个相加合并过程可由以下的公式计算得出：
\begin{equation}
	M_r=ReLU(\sum_{k=r\times g}^{(r+1)\times g-1}{M^k})
	\label{eq:Mr}
\end{equation}
公式\ref{eq:Mr}中$g$是每个组中的特征图的数量且 $g=K/B$，$r$的取值范围是$\{0,1,2,\dots,B-1\}$，即合并完成后一共有$B$张掩膜，$M_r$是第$r$组中合并完成后的掩膜，$M^k$是$\boldsymbol{M}$中第$k$个通道的特征图。

对于第二个缺陷，我们设计了一个去噪函数$f(m_{ij},\theta)$，该函数中$m_{ij}$是掩膜$M_r$中第$i$行第$j$列的像素值，$\theta$是一个百分比值。具体的去噪计算结果由以下公式计算得出：
\begin{equation}
	f(m_{ij},\theta)= \begin{cases}
		\ m_{ij}, & \rm if \enspace \it m_{ij}>p(M_r,\theta) ; \\
		\ 0, & \rm otherwise .\\
	\end{cases}
\label{eq:denoise}
\end{equation}
公式\ref{eq:denoise}中$p(M_r,\theta)$表示$M_r$的所有像素值从大到小处于$\theta$百分比的值，比如有100个像素值，每个像素值都是1到100中的一个整数取值且取值唯一，此时$\theta =70$，那么$p(M_r,\theta)=70$。\refeq{eq:denoise}公式的作用就是将掩膜$M_r$中百分比前$\theta$大的像素值保留，小于$p(M_r,\theta)$的像素值置为0。经过去噪操作后可以得到更加纯净的掩膜。

到目前为止，还剩下最后一个操作即可得到最终的掩膜。将$M_r$中的所有像素值进行最大最小归一化，使其像素值区间位于$[0,1]$，这样方便将掩膜直接和输入图片相乘进行扰动操作。具体计算公式如下：
\begin{equation}
	M^{\prime}_r=\frac{M_r-min(M_r)}{max(M_r)-min(M_r)}\label{e_minmax}
\end{equation}
最后，掩膜$M^{\prime}_r$有这和原始输入图片$I_0$一致的分辨率且掩膜的像素值和$I_0$中的像素值一一对应。

\subsection{生成显著图}
如果显著性方法确实识别出了对模型预测有重要意义的像素，那么这一点就应该反映在重建图像的模型输出当中\textsuperscript{\cite{kapishnikov2019xrai}}。将$M^{\prime}_r$作为掩膜来扰动原始输入图片$I_0$，其背后的原理是从原始输入图像中保留掩膜中获得的关于类别$c$的特征信息，让图像分类模型来判断保留的这部分信息的重要性，判断依据就是图像分类模型对扰动后的图片关于类别$c$的输出概率分数。但是，如果直接将掩膜和原始输入图像相乘进行扰动，那么扰动后的图像中的被掩盖区域和被凸显的区域之间的边界会过于明显锐利，从而对图像分类神经网络造成对抗效果\textsuperscript{\cite{dabkowski2017real}}。

为了遇到避免上述的问题，高斯模糊函数被引入到了接下来的改进方法中。具体方法是将扰动区域即被掩膜遮盖的区域用高斯模糊后的原始输入图片的对应区域进行替代，这样可以使得凸显的图片区域和被扰动的图片区域之间的边界更加平滑，从而像真实图片，不易让图像分类神经网络产生异常的输出。对于单一掩膜$M^{\prime}_r$，这个扰动模式可以由以下的公式计算得出：
\begin{equation}
	I_r=I_0\odot M^{\prime}_r+I^{\prime}_0\odot (1-M^{\prime}_r)
	\label{eq:Ir}
\end{equation}
公式\ref{eq:Ir}中，$I^{\prime}_0$是将原始输入图片$I_0$高斯模糊后的得到的，$I^{\prime}_0$作为一张基础图片送入图像分类模型$\mathcal{F}$当中，当然得到其关于类别$c$的输出结果$\mathcal{F}_c(I^{\prime}_0)$是一个非常低的值且趋近于0。具体的高斯模糊函数是$guassian\_blur2d(input,kernel\_size,sigma)$，在本章中，参照Group-CAM\textsuperscript{\cite{zhang2021novel}}的工作，设置高斯模糊的参数$kernel\_size=51$和$sigma=50$。 $1-M^{\prime}_r$表示$M^{\prime}_r$中每个像素值都作为减数被1相减，作用是将掩膜中关于类别$c$的特征信息进行凸显。

参考RISE\textsuperscript{\cite{petsiuk2018rise}}中的方法，每张掩膜$M_r$的权重$\alpha_r$可以由扰动后的$I_r$输入到图像分类模型$\mathcal{F}$中得到。$\mathcal{F}_c(I_r)$则是该权重，其表示模型对显著图区域中类别$c$的特征信息的感兴趣程度。具体计算公式由以下式子给出：
\begin{equation}
	\alpha_r=\mathcal{F}_c(I_r)-\mathcal{F}_c(I^{\prime}_0)
\end{equation}
最终的显著图即将所有$M_r$的权重$\alpha_r$和其本身相乘，并经过ReLU函数后得到，ReLU函数的作用是保留显著图中的正值，即模型关于类别$c$感兴趣的像素区域。具体式子如下表示：
\begin{equation}
	L_{MSG-CAM}=ReLU(\sum_{r}\alpha_r M_r)
\end{equation}
 

 
 
\section{实验与分析}

\subsection{实验硬件配置和软件环境}
下面给出本章实验的硬件配置和实验环境相关信息，具体信息见表\ref{tab:en}。
\begin{table}[h]
	\renewcommand{\arraystretch}{1.5}
	\centering
	\bicaption[\xiaosi 实验环境和硬件配置]{\wuhao 实验环境和硬件配置}{\wuhao Experimental environment and hardware configuration}
	\begin{tabular}{p{3cm}p{2.25cm}p{2.25cm}p{2.25cm}p{2.25cm}}
		\toprule[1.5pt]
		\makecell[c]{\songti\wuhao CPU}&\makecell[c]{\songti\wuhao GPU}&\makecell[c]{\songti\wuhao 操作系统}&\makecell[c]{\songti\wuhao Python版本}&\makecell[c]{\songti\wuhao PyTorch版本}\\
		\hline
		\makecell[c]{\wuhao Intel$^\circledR$ Xeon$^\circledR$\\ \wuhao W-2255 CPU\\ \wuhao@3.70GHz}&\makecell[c]{\wuhao NVIDIA \\ \wuhao RTX A5000}&\makecell[c]{ \wuhao  Ubuntu20.04}&\makecell[c]{\wuhao Python3.8}&\makecell[c]{\wuhao 1.10.1+cu113}\\
		\bottomrule[1.5pt]
	\end{tabular}
	\label{tab:en} 	
\end{table}

\subsection{数据集及其预处理和实验参数说明}
在本章的实验当中，主要使用了三个数据集，下面是三个数据集的简要介绍：
1. ILSVRC 2012数据集
ILSVRC 2012（ImageNet Large Scale Visual Recognition Challenge 2012）是一个用于视觉对象识别和定位的大规模数据集和竞赛。该数据集包含超过120万张标注图片，涵盖1000个不同类别的物体和场景。ILSVRC 2012竞赛旨在推动计算机视觉领域的发展，参与者需要开发能够识别图像中物体类别的算法，并对物体进行定位。该竞赛对于深度学习和卷积神经网络等技术的发展起到了重要推动作用，成为了评估图像识别算法性能的重要基准。ILSVRC 2012数据集的发布和竞赛对于推动计算机视觉领域的发展产生了深远影响。

2. PASCAL VOC数据集
PASCAL VOC（Visual Object Classes）数据集是一个常用的计算机视觉数据集，用于目标检测、图像分割和场景分类等任务。该数据集最初由牛津大学的计算机视觉研究组创建，包含了20个常见的物体类别，如人、狗、猫、飞机等。数据集中的图像来自于自然场景和网络图像，每个图像都标注了包含的物体类别和位置信息。PASCAL VOC数据集被广泛应用于评估目标检测和图像分割算法的性能，是计算机视觉领域中的重要基准数据集之一。同时，PASCAL VOC数据集也被用于举办国际性的计算机视觉竞赛，吸引了全球的研究者和工程师参与。通过使用PASCAL VOC数据集，研究人员可以开发和评估各种视觉任务的算法，推动计算机视觉技术的发展。

3. COCO2014数据集
COCO2014数据集（Common Objects in Context）是一个用于计算机视觉任务的大型数据集，包含超过200,000张图像和相关的注释信息。这些图像涵盖了80个不同类别的物体，并且每张图像都有多个物体的标注，这使得数据集在目标检测、图像分割和物体识别等任务中非常有用。COCO2014数据集的引入为计算机视觉领域的研究和发展提供了重要的资源，成为了许多视觉任务的标准基准。研究人员和开发者可以利用该数据集进行模型训练、算法评估和性能比较，从而推动计算机视觉技术的进步。COCO2014数据集的广泛应用促进了目标检测、图像分割和物体识别等领域的发展，为相关领域的研究和应用提供了有力支持。

在本章当中插入与删除实验中使用的是ILSVRC 2012验证集，包含50000张图片，并且将该验证集中的图片尺寸调整为(3$\times$224$\times$224)，其中$3$是表示RGB颜色通道数量，像素值进行归一化调整，调整后的像素值范围是[0, 1]，最后使用Imagenet数据集的均值[0.229, 0.224, 0.225]和方差[0.485, 0.456, 0.406]对所有图片像素值进行标准化处理。在定点游戏实验中，使用了两个数据集，分别是PASCAL VOC数据集中的测试集，包含4952张图片，和COCO 2014数据集的验证集，包含50000张图片。用于本章实验的图像分类神经网络模型是torchvision提供的预训练模型VGG19\textsuperscript{\cite{simonyan2014very}}，它是基于卷积神经网络架构的。如果没有特别说明，本章提出的方法MSG-CAM的默认设置参数是$B=32$，$\theta=70$。所有方法生成的最终显著图默认都上采样至224$\times$224的分辨率。
\subsection{定性评估}

\subsection{插入（insertion）和删除（deletion）实验}
插入（insertion）和删除（deletion）实验首次在RISE\textsuperscript{\cite{petsiuk2018rise}}实验中提出。插入实验背后所反映的原理是当我们按照显著图给出的像素重要程度优先级开始在原图的对应位置逐渐插入重要的像素直至完全插入所有像素，在此过程中记录每次插入操作时模型对指定类别给出的可能性概率分数。插入实验可以衡量显著图对像素重要性的排序是否与模型实际决策时关注的像素重要性排序一致。与之相反的是，删除实验则是按照显著图中给出的像素重要程度优先级逐渐从原输入图片中抹去对应的像素信息。和插入实验一样，删除实验也要求记录每次删除操作后模型对感兴趣类别给出的可能性分数。删除实验可以直观的呈现出缺失重要特征的像素信息后模型对感兴趣类别的置信程度下降情况。

具体而言，对于插入实验，我们将原输入图片高斯模糊化后作为画布，随后每次迭代过程中，我们按照显著图给出的像素优先级逐渐向画布中对应的位置中引入原输入图片的像素，每次迭代过程中记录模型对引入像素后的的图片关于指定类别的可能性概率分数。为了更加精确的反映显著图的像素优先级，我们每次迭代只引入约0.89\%(224$\times$2)的像素。和插入实验相对比，删除实验每次迭代将原输入图片中的像素逐渐替代为画布上的像素，直到输入图片被完全替换为画布为止。删除实验每次迭代仍然是变更约0.89\%的在原图中的像素。需

要特别说明的是，我们引入高斯模糊后的原输入图片作为画布是为了避免引入像素或者删除像素时产生过于锐利的边界，从而更加接近真实图片，避免产生对抗攻击的样本。此外，每次迭代记录的可能性分数都是经过softmax函数进行归一化后的数据。得到插入实验和删除实验每次迭代获得的分数后，我们使用概率分数随插入或者删除次数的曲线的曲线下面积（Area Under Curve)作为的作为量化指标。为了总体衡量插入实验和删除实验的优劣，我们使用总体（over-all）分数。按照上面关于插入实验和删除实验的描述，AUC(insertion)越高表明显著图越准确，同理，AUC(deletion)越低越好。因此，overall分数计算方式是AUC(insertion)-AUC(deletion)。图\ref{fig:IDCurve}展示了Grad-CAM、Score-CAM、Group-CAM和MSG-CAM为所选的4幅代表性图像生成了显著图以及相应的插入和删除曲线。对于删除曲线，更好的显著图解释方法给出的显著性图应尽可能快地下降，插入曲线与删除曲线正好相反。从图中可以看出本文提出的MSG-CAM在视觉解释效果以及插入删除曲线的表现来看均优于其他三种基于类激活映射的方法。


表\ref{auc}列出了在50000张图像上进行插入和删除的实验结果。虽然MSG-CAM在单项指标上并不突出，但在AUC(overll)这项指标上却是第一名，比第二名的 Score-CAM高出1.07\%。此外，为了探究图像分类神经网络对目标类别的置信程度与三个指标之间的关系。我们根据每幅图像的分类类别的的最大得分将其分为11组，并统计每组的AUC(insertion)，AUC(insertion)和AUC(overall)，统计数据绘制成了表\ref{IDO}。 从图中可以看出，当图像的分类类别的概率分数较低时，各种显著图解释方法之间很难拉开差距。随着图像的分类类别的概率分数的上升，MSG-CAM逐渐显示出优势。在图像的分类类别的概率分数$\geq 99.99$时，即当图像分类神经模型对这组图像中的分类类别有很高的置信度时，MSG-CAM在AUC(overall)有着明显领先。这表明，当图像分类神经网络接近完美的学习到相关类别信息时，MSG-CAM能准确反映这种情况。


\begin{figure}[h]
	\centering 
	\includegraphics[width=15cm]{fig/ch3/IDCurve.pdf}
	\bicaption[\xiaosi 单张图片在不同方法下的插入删除实验曲线对比]{\wuhao 不同方法下的插入删除实验曲线对比图}{\wuhao Comparison of experimental curves of insertion and deletion under different methods}
%	\bicaption[\xiaosi MSG-CAM算法流程示意图]{\wuhao MSG-CAM算法流程示意图}{\wuhao Pipeline of MSG-CAM}
	\label{fig:IDCurve}
\end{figure}


\begin{table}
	\renewcommand{\arraystretch}{1.5}
	\centering
	\bicaption[\xiaosi 电流类型对效率的影响]{\wuhao 电流类型对效率的影响}{\wuhao Current type impact on efficiency}
	\wuhao
	% \begin{threeparttable}
		\begin{tabularx}{\textwidth}{XXXXXXXX}%四个c代表该表一共四列，内容全部居中
			\toprule[1.5pt]
			AUC&GradCAM&GradCAM++&XGradCAM&ScoreCAM&GroupCAM&CAMERAS&\pmb{MSGCAM} \\
			\toprule[1.5pt]%第二道横线 
			Insertion&53.19&51.57&52.57&\pmb{55.10}&54.61&44.10&54.52 \\

			Deletion&11.52&12.16&11.53&11.43&11.21&\pmb{8.01}&9.78 \\

			Over-all&41.67&39.41&41.04&43.67&43.40&36.09&\pmb{44.74}\\
			\bottomrule[1.5pt]%第三道横线
		\end{tabularx}
\end{table}

\begin{table}
	\renewcommand{\arraystretch}{1.5}
	\centering
	\bicaption[\xiaosi 电流类型对效率的影响]{\wuhao 电流类型对效率的影响}{\wuhao Current type impact on efficiency}
	\wuhao
	\begin{tabular}{ccccccc} %需要10列
		\toprule[1.5pt] %添加表格头部粗线
		\multicolumn{2}{c}{\multirow{2}{*}{Method}}& \multicolumn{2}{c}{PASCAL VOC test}&&\multicolumn{2}{c}{COCO 2014 validation}\\
		\multicolumn{2}{c}{}&&Mean Accuracy($\%$)&&Mean Accuracy($\%$)&\\  %有n个&，就表示该行有n+1列
		\toprule[1.5pt] %绘制一条水平横线
		\multicolumn{2}{c}{Grad-CAM}&   &83.04&    &55.50&    \\   % 占两列，列名为A；后面陆续跟着数字
		\multicolumn{2}{c}{Grad-CAM++}&   &83.21&    &52.91&   \\
		\multicolumn{2}{c}{XGrad-CAM}&   &86.70&    &55.93&   \\
		\multicolumn{2}{c}{Score-CAM}&   &73.92&   &51.20&   \\
		\multicolumn{2}{c}{Group-CAM}&   &82.41&  &54.14&   \\
		\multicolumn{2}{c}{CAMERAS}&   &87.16&    &55.40&   \\
		\multicolumn{2}{c}{\pmb{MSG-CAM}}&   &\pmb{87.24}&   &\pmb{56.62}&   \\
		\bottomrule[1.5pt] %添加表格底部粗线
	\end{tabular}
	\label{point}
\end{table}

\begin{figure*}[htbp]
	\centering
	\bicaption[\xiaosi 单张图片在不同方法下的插入删除实验曲线对比]{\wuhao 不同方法下的插入删除实验曲线对比图}{\wuhao Comparison of experimental curves of insertion and deletion under different methods}
	\subfigure[Insertion score divides into segments]{
		\includegraphics[width=6cm]{fig/ch3/insertion.png}
	}
	\subfigure[deletion score divides into segments]{
		\includegraphics[width=6cm]{fig/ch3/deletion.png}
	}
	\subfigure[over-all score divides into segments]{
		\includegraphics[width=6cm]{fig/ch3/overall.png}
	}
	% \quad
	\label{IDO}
\end{figure*}



\subsection{定点游戏实验}
定点游戏（pointing game）\textsuperscript{\cite{zhang2018top}}实验对于给定显著图解释方法的空间选择性能够给出比较客观的评估结果。所谓空间选择性就是定点游戏要求图像分类神经网络对给定类别在输入图片中指出其所在位置，这对显著图解释方法提出了挑战，方法生成的显著图必须要准确反映给定类别在输入图片的具体位置。定点游戏的具体操作是从针对一个指定类别生成的显著图中找到值最大的那个点并记录它的坐标，如果该坐标位于该类别对象的预先人工标注的边界框内部，则记录一次击中（hit）,否则就是没有击中（miss）。定点游戏的量化评价指标即是显著图解释方法在给定数据集上对输入图片中所有类别的击中准确率，具体计算公式如下：
\begin{equation}
	Acc=\frac{\#hits}{\#hits+\#misses}
\label{eq:point}
\end{equation}
在式\ref{eq:point}中，$\#hits$表示数据集中所有显著图的最大值点落在标注框中的个数，$\#misses$则表示数据集中所有显著图的最大值点落在标注框外的个数。在只计算最大值空间选择性的定点游戏当中，$\#hits+\#misses$就等于数据集中所有图片标注框的个数。

为了更加精确的反映显著图的空间选择性，本节实验当中摈弃了只计算最大值点所在坐标的做法，转而计算显著图中里面从大到小的前100个点的坐标，这可以排除偶发性的噪声影响。最终的性能指标是对一个数据集中每个类别的$Acc$取平均。在本次实验中，我们使用的是VGG19模型，且分别在PASCAL VOC测试集和COCO 2014验证集上进行了微调训练。如表\ref{point}所示，我们的方法在两个数据集上都处于领先地位，这表明我们的方法比其他基于类激活映射图的方法更能反映图像分类神经网络的空间选择性。

\subsection{合理性检验}
部分基于反向传播的显著图方法对模型参数并不敏感，这意味不管图像分类模型有没有经过训练，它们都能够给出相似的结果，这显然是违背显著图解释的初衷。所以，能否通过合理性检验是衡量显著图解释方法是否具备可解释性的标准。具体而言，本节~\cite{zhang2018top}根据中的实验方法对经过预训练的VGG19采用级联随机化（cascade randomization）和独立随机化（independent randomization）。级联随机化是从靠近模型的输出端开始，逐渐将卷积层的参数随机化，直至将所有卷积层的参数随机化。 独立随机化也是从靠近模型的输出端开始，每次仅将单独的一层卷积层参数随机化，其他卷积层不变。

图\ref{fig:sanitycheck}中展示了MSG-CAM在单一图片下分别进行级联随机化和独立随机化的结果示意图，图中Conv34表示的是将VGG19的第34层卷积层经过独立随机化和级联随机化后的显著图生成结果，其他的Conv32等以此类推。可以看出无论是级联随机化还是独立随机化，本章提出的MSG-CAM生成的显著图均受到了明显影响，表面MSG-CAM是受到图像分类神经网络训练参数影响的，能够通过合理性检验。

\begin{figure}[h]
	\centering 
	\includegraphics[width=15cm]{fig/ch3/sanityCheck.pdf}
	\bicaption[\xiaosi MSG-CAM合理性检验]{\wuhao MSG-CAM合理性检验示意图}{\wuhao Sanity check of MSG-CAM}
	\label{fig:sanitycheck}
\end{figure}


\subsection{图}
下面给出图片示例：

%调整图片与上方文字之间的间距
%\vspace{-0.1cm}

\begin{figure}[h]
		\centering 
		\includegraphics[width=10cm]{chapters/31}
	    \bicaption[\xiaosi 不同缩放系数v的缩放效果]{\wuhao 不同缩放系数v的缩放结果}{\wuhao Scaling results with different scaling coefficients ν}
	   	 \label{fig:3.1}
\end{figure}

%调整图片与下方文字之间的间距
%\vspace{-0.35cm}

图片标题与图片之间的间距使用默认设置即可，与上下文的间距由于LATEX动态排版特性，需要大家手动调整。

。

。

。

。

。

。

。

。



下图是多子图示例：
%\vspace{-1cm}



\begin{figure}[h]
	\centering
	\subfigure[]{
		\label{fig:DE_J}
		\includegraphics[width=12cm]{chapters/DE_J.pdf}}
	\subfigure[]{
		\label{fig:DE_CF}
		\includegraphics[width=12cm]{chapters/DE_CF.pdf}}   
\bicaption[\xiaosi 理论效率与$\gamma$和$\varphi$的关系。]{\wuhao 理论效率与$\gamma$和$\varphi$的关系。 (a) $\alpha=1$; (b) $\alpha=2/\sqrt{3}$}{\wuhao Theoretical DE versus $\gamma$ and $\varphi$. (a) $\alpha=1$; (b) $\alpha=2/\sqrt{3}$}

%	\caption{\wuhao 理论效率与$\gamma$和$\varphi$的关系。 (a) $\alpha=1$; (b) $\alpha=2/\sqrt{3}$}
%%	\raggedright
%	\wuhao Fig. 3-2 Theoretical DE versus $\gamma$ and $\varphi$. (a) $\alpha=1$. (b) $\alpha=2/\sqrt{3}$.Theoretical DE versus $\gamma$ and $\varphi$. (a) $\alpha=1$. (b) $\alpha=2/\sqrt{3}$.
\end{figure}

\vspace{-0.5cm}

\subsection{表}

表格格式参照写作指南。表格格式参照写作指南。表格格式参照写作指南。表格格式参照写作指南。表格格式参照写作指南。表格格式参照写作指南。表格格式参照写作指南。表格格式参照写作指南。表格格式参照写作指南。表格格式参照写作指南。表格格式参照写作指南。表格格式参照写作指南。表格格式参照写作指南。表格格式参照写作指南。表格格式参照写作指南。表格格式参照写作指南。

\vspace{0.1cm}

\begin{table}[h]
	\renewcommand{\arraystretch}{1.5}
	\centering
	\bicaption[\xiaosi 电流类型对效率的影响]{\wuhao 电流类型对效率的影响}{\wuhao Current type impact on efficiency}
	\begin{tabular}{p{3cm}p{3cm}p{3cm}p{3cm}}
		\toprule
		\makecell[c]{\songti\wuhao 电流类型}&\makecell[c]{\songti\wuhao A}&\makecell[c]{\songti\wuhao B}&\makecell[c]{\songti\wuhao C}\\
		\hline
		\makecell[c]{\wuhao aaa}&\makecell[c]{\wuhao aa1}&\makecell[c]{\wuhao bb1}&\makecell[c]{\wuhao cc1}\\
		\bottomrule
	\end{tabular}
   \label{tab:3.1} 	
\end{table}

表格格式参照写作指南。表格格式参照写作指南。表格格式参照写作指南。表格格式参照写作指南。表格格式参照写作指南。表格格式参照写作指南。表格格式参照写作指南。表格格式参照写作指南。表格格式参照写作指南。表格格式参照写作指南。表格格式参照写作指南。表格格式参照写作指南。表格格式参照写作指南。表格格式参照写作指南。表格格式参照写作指南。表格格式参照写作指南。

\vspace{-0.1cm}

\begin{table*}[h]
	\renewcommand{\arraystretch}{1.5}
	\bicaption[\xiaosi 高效率功放性能对比]{\wuhao 高效率功放性能对比}{\wuhao High-effiency power amplifier performance comparison}
	\label{tab_1}
	\centering
	\wuhao
	\begin{tabular}{c c c c c }
		\hline
		{\textbf{带宽}(GHz)}&{\textbf{功率}(dBm)}&{\textbf{效率}(\%)}&{\textbf{线性度}(dBc)}&{\textbf{信号带宽}(MHz)}\\
		\hline
		1.4--2.6&32--34&30--40 (DE)&-25 -- -30 (ACLR)&5\\
		\hline
		\multirow{2}{*}{2.1--2.7}&39&45 (DE) @ 2.14 GHz&--31 (ACLR)&\multirow{2}{*}{5}\\\cline{3-4}
		&(average)&40 (DE) @ 2.655 GHz&--30 (ACLR)&\\
		\hline
		3.5&38.1&59 (PAE)&30 (C/I)&5\\
		\hline
		\multirow{2}{*}{1.6--2.6}&36.0--38.5&45--60 (PAE)&30 (C/I)&5\\\cline{2-5}
		&35.3--37.5&40--55 (PAE)&--30 (ACLR)&20\\
		\hline
	\end{tabular}
\end{table*}


\section{公式格式}

\begin{equation}
\left\{ \begin{aligned}
0.794 \le \zeta  \le 1 ~~~~~~~~~~~\\
0.631 \le \gamma  = \frac{{0.631}}{{{\zeta ^2}}} \le 1~~~~~~ \\
- \frac{1}{{2\gamma }} \le \delta  \le \frac{1}{{2\gamma }}~~~~~~~~~~~ \\
{Z_{c,low,f}} = 2{R_{opt}}(\gamma  + j\delta )~~~~~\\
{Z_{c,2f}} = {Z_{c,low,2f}} =  - j\frac{{3\pi }}{4}\gamma \delta {R_{opt}}
\end{aligned} \right.
\label{eq:3.1}
\end{equation}

\begin{equation}
\begin{aligned}
v(\theta ) = V_{DD}\cdot(1 - \alpha cos(\theta  + \varphi ) + \beta cos(3\theta  + 3\varphi ))\\
\cdot(1 - \gamma \sin (\theta  + \varphi )) ~~~~~- 1 \le \gamma  \le 1\
\end{aligned}
\label{eq:vd}
\end{equation}




\noindent
公式格式测试。${\mathbf{\Theta }} = \left\{ {{\theta _k}\left( n \right),\forall k,n} \right\}$

\section{印制要求}
涉密学位论文的印刷、制作、传递、存档等，须符合国家、学校相关保密要求。学位论文一律左侧装订。

中文摘要之前的前置部分（封面、中、英文题名页、独创性声明和使用授权书），采用单面印刷。

从中文摘要开始，采用双面印刷。

中文摘要及之后的前置部分，包括中文摘要、ABSTRACT、目录、图目录（如有）、表目录（如有）、主要符号表（如有）、缩略词表（如有），在双面印刷时，若某部分页数为奇数，则该部分最后一页单面印刷。例如：若“摘要”只有1页，则其页码是“Ⅰ”，第“Ⅰ”页纸的背面为空白（无页眉或页码）；“ABSTRACT”用新的一张纸印刷，页码从“Ⅱ”开始。

从第1章第1页开始，至论文最后1页，所有页面均双面印刷。例如：若第1章的最后1页为第17页，则第2章的第1页在第17页的背面印刷，页码为“18”（页眉是“重庆邮电大学博士（硕士）学位论文”）。

一次性双面打印整本学位论文技巧：除用于打印的版本外，电子版论文中一律不得出现空白页。论文打印建议使用PDF格式。为方便一次性双面打印，有时可在单面印刷的部分（如封面、中、英文题名页、独创性声明和使用授权书），或者双面打印只有1页的某部分内容（如摘要、ABSTRACT等）后插入1页空白页，该空白页不编排页眉页码；论文中出现的页码应前后连续，不得中断。


\section{本章小结}
本章介绍了……