%英文摘要，自行编辑内容




\chapter{ABSTRACT}
\xiaosi

In various fields, deep learning technologies have demonstrated their application potential. In the field of image classification, as model structures become increasingly complex and the number of parameters rises, it becomes challenging to provide reliable explanations for the classification results obtained by image classification models. Saliency map interpretation, as an emerging branch of interpretable deep learning work, aims to identify the basis for the decisions made by image classification models from input images and provide visual explanations. However, current saliency map interpretation techniques commonly suffer from low resolution of generated original saliency maps and shortcomings in accurately locating key target features. Therefore, this thesis addresses the deficiencies in current saliency map interpretation techniques by investigating methods to enhance the resolution of saliency maps, increase the effective information presented in saliency maps, and improve the visual explanatory capability of saliency maps for the decisions made by image classification neural networks. The main objectives of this thesis are as follows:

First, This thesis proposes a saliency map interpretation method for image classification models based on convolutional neural networks. This method involves scaling the original input images at multiple resolutions and feeding them separately into the image classification model to obtain a collection of feature maps at different resolutions from the last convolutional layer of the image classification neural network. The scores of the target class are back-propagated to obtain the gradient matrix corresponding to the feature maps of the last convolutional layer. Subsequently, the feature maps at different resolutions and the gradient matrix are fused to the same resolution as the original input image and weighted summated to generate masks. By perturbing the original input images with each mask and inputting them into the image classification model, the probability scores of the target class are obtained as the weights of the masks. Finally, the masks and their corresponding probability scores are weighted multiplied to obtain the final saliency map. Qualitative and quantitative experiments on three public datasets demonstrate that this method produces saliency maps with higher resolution, enabling more precise identification of the decision basis of image classification neural networks in input images and providing more intuitive visual explanatory results.


Second, addressing the common issue of low-resolution original saliency maps generated by current saliency methods, this thesis introduces a universal saliency map enhancement method that can be directly applied to most saliency methods. This method utilizes a fixed-size sliding window to upsample all local regions in the input image to the input image size, which are then fed into the selected saliency map interpretation method to obtain saliency maps and probability scores for specific classes for all images. Subsequently, the saliency maps are downsampled to the corresponding positions of the windows in the input images and multiplied by the probability scores to generate saliency maps with more details. By applying this method to different saliency methods, both quantitative metrics and visual evaluations demonstrate a significant improvement in the quality of saliency maps generated by other saliency methods, thereby confirming the effectiveness and reliability of this method.

Finally, in response to the complexity of current numerous saliency map generation methods and the challenge of intuitively comparing the quality of saliency maps produced by different methods, this thesis designs and implements a saliency map interpretation method comparative evaluation system.
\\

\noindent\textbf{Keywords：} 
\begin{minipage}[t]{0.85\linewidth}
	Image Classification Neural Networks, Deep Learning Interpretability, Saliency Maps, Visualization
\end{minipage}

\clearpage